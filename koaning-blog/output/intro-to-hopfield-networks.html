<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Intro to Hopfield Networks â€” koaning.io</title>
	<meta name="description" content="Title: Intro to Hopfield Networks; Date: 2015-07-23; Author: Vincent D. Warmerdam">
	<meta name="author" content="Vincent D. Warmerdam">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" type="text/css" href="/theme/css/bootstrap.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/style.css">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <script src="http://cdnjs.cloudflare.com/ajax/libs/d3/3.4.1/d3.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lodash.js/3.10.1/lodash.js">
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
	</head>
<body>
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <br>
    		<h1><a class="web_title" href="/">koaning.io</a></h1>
        <blockquote>Blog of a data scientist/developer</blockquote>
    		<br><br>
      </div>
    </div>
  	<div class="row">
  		<div class="col-md-12">
  			<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="text-center article-header">
		<h1 itemprop="name headline" class="article-title">Intro to Hopfield Networks</h1>
		<span itemprop="author" itemscope itemtype="http://schema.org/Person"></span>
		<time datetime="2015-07-23T00:00:00" itemprop="datePublished">Thu 23 July 2015</time>
	</div>
	<br><br>
		<div itemprop="articleBody" class="article-body"><p>In this document I will show how a hopfield recurrent network works by building one with numpy. First we'll create a hello world example, after which we'll create a method that will use these networks on simple images.</p>
<blockquote>
<p>If preferable, instead of reading here, you can also view this post as an ipython notebook <a href="http://nbviewer.ipython.org/gist/anonymous/01df7e791c1b2cc46baf">here</a>. </p>
</blockquote>
<h3>A quick demo</h3>
<p>The idea behind a hopfield network is to supply it with 'memories' where it can learn patterns from. It will not remember the values of the memories but will learn from the correlations within the data itself. Maybe this code can explain it better than words can. </p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">os</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">],[</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="mi">1</span><span class="p">]]</span>

<span class="k">def</span> <span class="nf">learn</span><span class="p">(</span><span class="n">learn_data</span><span class="p">):</span>
    <span class="n">k</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">learn_data</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="n">hopfield</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">zeros</span><span class="p">([</span><span class="n">k</span><span class="p">,</span><span class="n">k</span><span class="p">])</span>
    <span class="k">for</span> <span class="n">in_data</span> <span class="ow">in</span> <span class="n">learn_data</span><span class="p">:</span> 
        <span class="n">np_arr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="kp">matrix</span><span class="p">(</span><span class="n">in_data</span><span class="p">)</span>
        <span class="n">lesson</span> <span class="o">=</span> <span class="n">np_arr</span><span class="o">.</span><span class="n">T</span><span class="o">*</span><span class="n">np_arr</span>
        <span class="n">np</span><span class="o">.</span><span class="n">fill_diagonal</span><span class="p">(</span><span class="n">lesson</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>
        <span class="n">hopfield</span> <span class="o">=</span> <span class="n">hopfield</span> <span class="o">+</span> <span class="n">lesson</span>
    <span class="k">return</span> <span class="n">hopfield</span>

<span class="n">hopfield_matrix</span> <span class="o">=</span> <span class="n">learn</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
</pre></div>


<p>We take two inputs which we learn from and create a matrix containing co-occurence information. This resulting hopfield matrix contains the following: </p>
<div class="highlight"><pre><span class="k">[[ 0. -2.  2. -2.  2. -2.]</span>
 <span class="k">[-2.  0. -2.  2. -2.  2.]</span>
 <span class="k">[ 2. -2.  0. -2.  2. -2.]</span>
 <span class="k">[-2.  2. -2.  0. -2.  2.]</span>
 <span class="k">[ 2. -2.  2. -2.  0. -2.]</span>
 <span class="k">[-2.  2. -2.  2. -2.  0.]]</span>
</pre></div>


<p>Note that this matrix has learned from the alternating series. Which can also be visualized in graph form. </p>
<p><img alt="" src="http://i.imgur.com/xKWDnW1.png" /></p>
<p>This matrix can be multiplied with an input vector which will result in a new array that should closed resemble of the input memories. </p>
<div class="highlight"><pre>def normalize(res):
    res[res &gt; 0] = 1
    res[res &lt; 0] = -1
    return res

res = hopfield_matrix * np.matrix([1,1,1,1,-1,1]).T
normalize(res)
</pre></div>


<p>I apply normalisation to make sure the number stay either +1 or -1. The result makes sense: </p>
<div class="highlight"><pre>[-1,1,-1,1,-1,1]
</pre></div>


<p>Let's now learn such a network on some images with digits. </p>
<h3>To simple images</h3>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="kn">as</span> <span class="nn">plt</span>
<span class="o">%</span><span class="n">matplotlib</span> <span class="n">inline</span>  
<span class="kn">import</span> <span class="nn">cv2</span>

<span class="n">fig</span><span class="p">,</span> <span class="p">(</span><span class="n">ax1</span><span class="p">,</span> <span class="n">ax2</span><span class="p">)</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">)</span>
<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span> <span class="o">+</span> <span class="s">&#39;/hamilton_images/0.jpg&#39;</span><span class="p">)</span>
<span class="n">ax1</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>

<span class="n">img</span> <span class="o">=</span> <span class="n">cv2</span><span class="o">.</span><span class="n">imread</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span> <span class="o">+</span> <span class="s">&#39;/hamilton_images/1.jpg&#39;</span><span class="p">)</span>
<span class="n">ax2</span><span class="o">.</span><span class="n">imshow</span><span class="p">(</span><span class="n">img</span><span class="p">)</span>
</pre></div>


<p><img alt="" src="http://i.imgur.com/hIQakIV.png" /></p>
<p>Again, these images will serve us as memories, not labels. This makes a hopfield neural network different from classification or clustering methods. We are doing neither but something very similar. Let's try and get our hamilton network to learn the patterns from the images. </p>
<p>We will redefine the <code>learn</code> method to work for images now. </p>
<div class="highlight"><pre>def learn(learn_data):
    k = len(learn_data[0])
    hamilton = np.zeros([k,k])
    for in_data in learn_data:
        np_arr = np.matrix(in_data)
        lesson = np_arr*np_arr.T
        np.fill_diagonal(lesson, 0)
        hamilton = hamilton + lesson
    return hamilton

inputs = [] 
for i in [&#39;0&#39;, &#39;1&#39;]:
    img = cv2.imread(os.getcwd() + &#39;/hamilton_images/&#39; + str(i) + &#39;.jpg&#39;)
    an_input = normalize(np.sum(img, axis=2) - 766/2.0)
    an_input.resize((50*50,1))
    inputs.append(list(an_input))

hamilton = learn(inputs)
</pre></div>


<p>This learning might take a while. It is interesting to see the hamiltonian matrix. Certain regions in the image don't matter in distringuishing the images. </p>
<div class="highlight"><pre>plt.imshow(hamilton)
</pre></div>


<p><img alt="" src="http://i.imgur.com/WR9MLDt.png" /></p>
<p>Now we'll creata a method that can take a noisy image and it will try to find the right memory. </p>
<div class="highlight"><pre>def network_output(problem):
    an_input = normalize(np.sum(problem, axis=2) - 766/2.0)
    an_input.resize((problem.shape[0]*problem.shape[1],1))
    output = hamilton * an_input
    output.resize(problem.shape[0],problem.shape[1])

    pltr = np.zeros(img.shape) 
    for i in range(pltr.shape[0]):
        for j in range(pltr.shape[1]):
            for k in range(pltr.shape[2]):
                pltr[i,j,k] = output[i,j]

    return normalize(pltr)

def recurse(f, i, n):
    if n == 1: 
        return f(i)
    return f(recurse(f, i, n - 1))
</pre></div>


<p>Note that I am normalizing the network output to force pixel to either be black or white. Now let's try out our method on a few images. </p>
<div class="highlight"><pre>fig, axis = plt.subplots(2,6)
fig.set_size_inches(12,5)
for i, problem in enumerate([&#39;01&#39;, &#39;02&#39;, &#39;11&#39;, &#39;12&#39;, &#39;n1&#39;, &#39;n2&#39;]):
    problem = cv2.imread(os.getcwd() + &#39;/hamilton_images/&#39; + problem + &#39;.jpg&#39;)
    axis[0][i].imshow(problem)
    axis[1][i].imshow(network_output(problem))
</pre></div>


<p>Let's have a look at the output. </p>
<p><img alt="" src="http://i.imgur.com/porrx76.png" /></p>
<p>The first four images behave just as expected.</p>
<p>The last two images might seem counter intuitive. The 5th image contains a lot of whitespace and the original 1 image contains more whitespace than the zero. So it might feel likely that the 5th image is a one. But then why is the 6th image also a one? </p>
<p>It's because the hopfield network doesn't care about a pixel being black or white, it only cares about the correlation between pixels. This means that all black pixel are the same as all input pixels. This may give unexpected side effects but this is not unwanted behavior per se. </p>
<p>Hopfield networks are fun. They offer an alternative way to think about neural networks and give insight in why having recurrence in such a network can be a good thing. </p>
<p>Again, the ipython notebook file for this can be found <a href="http://nbviewer.ipython.org/gist/anonymous/01df7e791c1b2cc46baf">here</a>. </p></div>
	<hr>
</div>
        <br>
  		</div>
  	</div> 
  </div>

  <div class="container">
  	<div class="row">
  		<div class="col-md-12 text-center center-block aw-bottom">
  			<p>Vincent D. Warmerdam Made with Pelican</p>
  		</div>
  	</div>
  </div>
  <!-- JavaScript -->
  <script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
  <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
  <script type="text/javascript">
  jQuery(document).ready(function($) {
  	$("div.collapseheader").click(function () {
  		$header = $(this).children("span").first();
  		$codearea = $(this).children(".input_area");
  		$codearea.slideToggle(500, function () {
  			$header.text(function () {
  				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
  			});
  		});
  	});
  });
  
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-28011256-6', 'auto');
  ga('send', 'pageview');

</script>
  </body>
</html>