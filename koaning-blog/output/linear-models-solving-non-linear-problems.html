<!DOCTYPE html>
<html lang="en">
<head>
	<meta charset="utf-8">
	<title>Linear Models Solving Non Linear Problems â€” koaning.io</title>
	<meta name="description" content="Title: Linear Models Solving Non Linear Problems; Date: 2015-01-09; Author: Vincent D. Warmerdam">
	<meta name="author" content="Vincent D. Warmerdam">
	<meta name="viewport" content="width=device-width, initial-scale=1.0">
  <link rel="stylesheet" type="text/css" href="/theme/css/bootstrap.css">
  <link rel="stylesheet" type="text/css" href="/theme/css/style.css">
  <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">
  <script src="http://cdnjs.cloudflare.com/ajax/libs/d3/3.4.1/d3.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/lodash.js/3.10.1/lodash.js">
  <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  </script>
	</head>
<body>
  <div class="container">
    <div class="row">
      <div class="col-md-12">
        <br>
    		<h1><a class="web_title" href="/">koaning.io</a></h1>
        <blockquote>Blog of a data scientist/developer</blockquote>
    		<br><br>
      </div>
    </div>
  	<div class="row">
  		<div class="col-md-12">
  			<div class="article" itemscope itemtype="http://schema.org/BlogPosting">
	<div class="text-center article-header">
		<h1 itemprop="name headline" class="article-title">Linear Models Solving Non Linear Problems</h1>
		<span itemprop="author" itemscope itemtype="http://schema.org/Person"></span>
		<time datetime="2015-01-09T00:00:00" itemprop="datePublished">Fri 09 January 2015</time>
	</div>
	<br><br>
		<div itemprop="articleBody" class="article-body"><p>In this document I will illustrate that how a logistic regression can hold up against a support vector machine in a situation where you would expect a support vector machine to perform better. Typically logistic regression fails due to the XOR phenomenon that can occur in data, but there is a trick around it.</p>
<p>The goal of this document is to convince you that you may need to worry more about the features that go into a model, less about which model to pick and how to tune it.</p>
<h2>Dependencies</h2>
<p>For this experiment I will use python and I'll assume that the following libraries are loaded: </p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span> 
<span class="kn">import</span> <span class="nn">pandas</span> <span class="kn">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">patsy</span> 
<span class="kn">from</span> <span class="nn">ggplot</span> <span class="kn">import</span> <span class="o">*</span> 
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.svm</span> <span class="kn">import</span> <span class="n">SVC</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">confusion_matrix</span>
</pre></div>


<h2>Generate demo data</h2>
<div class="highlight"><pre>da = np.random.multivariate_normal([1,1], [[1, 0.7],[0.7, 1]], 300)
dfa = pd.DataFrame({&#39;x1&#39;:da[:,0], &#39;x2&#39;: da[:,1], &#39;type&#39; : 1})
db1 = np.random.multivariate_normal([3,0], [[0.2, 0],[0, 0.2]], 150)
dfb1 = pd.DataFrame({&#39;x1&#39;:db1[:,0], &#39;x2&#39;: db1[:,1], &#39;type&#39; : 0})
db2 = np.random.multivariate_normal([0,3], [[0.2, 0],[0, 0.2]], 150)
dfb2 = pd.DataFrame({&#39;x1&#39;:db2[:,0], &#39;x2&#39;: db2[:,1], &#39;type&#39; : 0})
df = pd.concat([dfa, dfb1, dfb2])

ggplot(aes(x=&#39;x1&#39;, y=&#39;x2&#39;, color=&quot;type&quot;), data=df) + geom_point() + ggtitle(&quot;sampled data&quot;)
</pre></div>


<p>This should show a dataset similar to this one: </p>
<p><img alt="" src="/theme/images/non-linear-data.png" /></p>
<p>We've generated a classification problem that is impossible to split linearly. Typically we would expect the (linear) logistic regression to perform poorly here and we would expect a (non-linear) support vector machine to perform well.</p>
<div class="highlight"><pre>y,X = patsy.dmatrices(&quot;type ~ x1 + x2&quot;, df)
</pre></div>


<p>Let's see how both models perform now. </p>
<h3>Logistic Regression</h3>
<div class="highlight"><pre>pred = LogisticRegression().fit(X,ravel(y)).predict(X)
confusion_matrix(y,pred)
</pre></div>


<h6>Output:</h6>
<div class="highlight"><pre>array([[223,  77],
        [118, 182]])
</pre></div>


<h3>Support Vector Machine</h3>
<div class="highlight"><pre>pred = SVC().fit(X,ravel(y)).predict(X)
confusion_matrix(y, pred)
</pre></div>


<h6>Output:</h6>
<div class="highlight"><pre>array([[294,   6],
       [  2, 298]])
</pre></div>


<p>The SVM performs much better than the LR. How might we help this? </p>
<h2>A dirty trick</h2>
<p>Let's see if we can help the logistic regression out a bit. Maybe if we combine <code>x1</code> and <code>x2</code> into something nonlinear we may be able capture the problem in this particular dataset better. </p>
<div class="highlight"><pre>df[&#39;x1x2&#39;] = df[&#39;x1&#39;] * df[&#39;x2&#39;]
y,X = patsy.dmatrices(&quot;type ~ x1 + x2 + x1x2&quot;, df)
pred = LogisticRegression().fit(X,ravel(y)).predict(X)
confusion_matrix(y,pred)
</pre></div>


<h6>Output:</h6>
<div class="highlight"><pre>array([[290,  10],
       [  3, 297]])
</pre></div>


<p>I am feeding different data to the logistic regression, but by combining x1 and x2 we have suddenly been able to get a non-linear classification out of a linear model. I am still using the same dataset however, which goes to show that being creative with your data features can have more of an effect than you might expect.</p>
<p>Notice that the support vector machine doesn't show considerable improvement when applying the same trick.</p>
<div class="highlight"><pre>pred = SVC().fit(X,ravel(y)).predict(X)
confusion_matrix(y, pred)
</pre></div>


<h6>Output:</h6>
<div class="highlight"><pre>array([[294,   6], 
       [  2, 298]])
</pre></div>


<h1>Conclusion</h1>
<p>Why is this trick so useful?</p>
<p>You can apply a little bit more statistical theory to the regression model which is something a lot of clients (especially those who believe in econometrics) find very comforting. It is less a black box and feels more like a simple regression. Notice that factorization machines are powerfull because they use a very similar approach.</p>
<p>The main lesson here is, before you judge a method useless, it might be better to worry about not putting useless data in it.</p></div>
	<hr>
</div>
        <br>
  		</div>
  	</div> 
  </div>

  <div class="container">
  	<div class="row">
  		<div class="col-md-12 text-center center-block aw-bottom">
  			<p>Vincent D. Warmerdam Made with Pelican</p>
  		</div>
  	</div>
  </div>
  <!-- JavaScript -->
  <script src="https://code.jquery.com/jquery-2.1.1.min.js"></script>
  <script src="//maxcdn.bootstrapcdn.com/bootstrap/3.2.0/js/bootstrap.min.js"></script>
  <script type="text/javascript">
  jQuery(document).ready(function($) {
  	$("div.collapseheader").click(function () {
  		$header = $(this).children("span").first();
  		$codearea = $(this).children(".input_area");
  		$codearea.slideToggle(500, function () {
  			$header.text(function () {
  				return $codearea.is(":visible") ? "Collapse Code" : "Expand Code";
  			});
  		});
  	});
  });
  
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-28011256-6', 'auto');
  ga('send', 'pageview');

</script>
  </body>
</html>